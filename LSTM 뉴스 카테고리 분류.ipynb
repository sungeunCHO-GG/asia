{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Kosmo_24'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 라이브러리 임포트\n",
    "\n",
    "from keras.datasets import reuters  # 로이터 뉴스기사 데이터 임포트\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.preprocessing import sequence # 기사별 단어 숫자 통일\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기타 라이브러리 임포트\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed값 설정\n",
    "seed=0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로이터 뉴스 데이터 가져온 후, 단어 기준으로 자르기(toknize)하고\n",
    "# 불러 온 데이터를 학습셋, 테스트셋으로 나누기\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = reuters.load_data(num_words=1000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "8982\n",
      "2246\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "print(len(X_test))\n",
    "print(len(Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kosmo_24\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # natural language tool kit\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요', '저의', '이름은', '조성은입니다', '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 형태소 분석기 사용\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"안녕하세요 저의 이름은 조성은입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlpy 사용하면 한글 형태소 분석, mecab이 최근 핫함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 8982 개\n",
      "46 카테고리\n"
     ]
    }
   ],
   "source": [
    "# 카테고리 개수 세기\n",
    "\n",
    "print(\"총\", len(Y_train), \"개\") # 중복 포함 카테고리 수 \n",
    "category = np.max(Y_train) + 1\n",
    "print(category, '카테고리')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 학습용 뉴스 기사\n",
      "2246 학습용 뉴스 기사\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인하기\n",
    "\n",
    "print(len(X_train), \"학습용 뉴스 기사\")\n",
    "print(len(X_test), \"학습용 뉴스 기사\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 43,\n",
       " 10,\n",
       " 447,\n",
       " 5,\n",
       " 25,\n",
       " 207,\n",
       " 270,\n",
       " 5,\n",
       " 2,\n",
       " 111,\n",
       " 16,\n",
       " 369,\n",
       " 186,\n",
       " 90,\n",
       " 67,\n",
       " 7,\n",
       " 89,\n",
       " 5,\n",
       " 19,\n",
       " 102,\n",
       " 6,\n",
       " 19,\n",
       " 124,\n",
       " 15,\n",
       " 90,\n",
       " 67,\n",
       " 84,\n",
       " 22,\n",
       " 482,\n",
       " 26,\n",
       " 7,\n",
       " 48,\n",
       " 4,\n",
       " 49,\n",
       " 8,\n",
       " 864,\n",
       " 39,\n",
       " 209,\n",
       " 154,\n",
       " 6,\n",
       " 151,\n",
       " 6,\n",
       " 83,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 155,\n",
       " 11,\n",
       " 15,\n",
       " 7,\n",
       " 48,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 504,\n",
       " 6,\n",
       " 258,\n",
       " 6,\n",
       " 272,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 134,\n",
       " 44,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 8,\n",
       " 197,\n",
       " 2,\n",
       " 90,\n",
       " 67,\n",
       " 52,\n",
       " 29,\n",
       " 209,\n",
       " 30,\n",
       " 32,\n",
       " 132,\n",
       " 6,\n",
       " 109,\n",
       " 15,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 뉴스 기사 살펴보기\n",
    "\n",
    "print(len(X_train[0])) # 단어 개수가 87개가 들어있는 뉴스기사 \n",
    "X_train[0]\n",
    "\n",
    "# 로이터 데이터는 tokenize()함수를 이용해서 뉴스기사를 형태소 분석한 후에 단어 빈도수 \n",
    "\n",
    "# 딥러닝은 단어 그대로학습 x, 숫자로 변환한 다음에 학습해야 한다.\n",
    "\n",
    "# 여기서는 데이터 안에서 해당 단어의 빈도수 순위를 붙인 것. 1이면 첫번째 빈도수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3,  3,  4,  4, 19,  8, 16,  3,  3, 21, 11,  4,  4,  3,  3,  1,  3,\n",
       "        1,  3, 16,  1,  4, 13, 20,  1,  4,  4, 11,  3,  3,  3, 11, 16,  4,\n",
       "        4, 20], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Y_train))\n",
    "Y_train[10:46]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 기사 -> 87개 단어, 다른 것들은 150단어가 되면 형평성 x\n",
    "\n",
    "단어 수 맞추기 100개로 맞추기 \n",
    "\n",
    "sequence 라이브러리(클래스)의 pad_sequence 메소드(함수)를 사용함\n",
    "(라이브러리 = 패키지 = 클래스 = 모듈) 가져다쓰는 파이썬 코드 (메소드 = 함수)\n",
    "\n",
    "100개 단어 이상인 뉴스기사는 100개로 자르고, 100개 이상은 버린다.\n",
    "\n",
    "100개 단어 이하인 뉴스기사는(87개) 13개는 0으로 채운다. -> max_len=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(X_train, maxlen=100)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         1,   2,   2,   8,  43,  10, 447,   5,  25, 207, 270,   5,   2,\n",
       "       111,  16, 369, 186,  90,  67,   7,  89,   5,  19, 102,   6,  19,\n",
       "       124,  15,  90,  67,  84,  22, 482,  26,   7,  48,   4,  49,   8,\n",
       "       864,  39, 209, 154,   6, 151,   6,  83,  11,  15,  22, 155,  11,\n",
       "        15,   7,  48,   9,   2,   2, 504,   6, 258,   6, 272,  11,  15,\n",
       "        22, 134,  44,  11,  15,  16,   8, 197,   2,  90,  67,  52,  29,\n",
       "       209,  30,  32, 132,   6, 109,  15,  17,  12])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# y값 전처리 : 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y데이터에 원핫 인코딩\n",
    "y_train=np_utils.to_categorical(Y_train)\n",
    "y_test=np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 구조 짜기(LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 기본 뼈대 짜기\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(불러온 단어 총 개수, 기사당 단어 수))\n",
    "model.add(기사 당 단어 수, 기타 옵션) # LSTM이 RNN의 gradient 문제 해결\n",
    "model.add(Dense())\n",
    "\n",
    "Embedding : 총 1000개의 단어 중 100개의 단어가 들어옴\n",
    "LSTM : 100개 노드(기사당 단어 수), activation: tanh\n",
    "Dense: 출력 46개 노드(카테고리 수), activation: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 설정\n",
    "model=Sequential()\n",
    "model.add(Embedding(1000, 100)) # 기사 단어 수는 100개\n",
    "model.add(LSTM(100, activation='tanh'))\n",
    "model.add(Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/20\n",
      "8982/8982 [==============================] - 14s 2ms/step - loss: 2.5574 - accuracy: 0.3670 - val_loss: 2.0936 - val_accuracy: 0.4938\n",
      "Epoch 2/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 2.0053 - accuracy: 0.4890 - val_loss: 1.9193 - val_accuracy: 0.5098\n",
      "Epoch 3/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.9804 - accuracy: 0.5035 - val_loss: 1.9805 - val_accuracy: 0.5116\n",
      "Epoch 4/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.7963 - accuracy: 0.5450 - val_loss: 1.7401 - val_accuracy: 0.5695\n",
      "Epoch 5/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.7064 - accuracy: 0.5639 - val_loss: 1.8843 - val_accuracy: 0.5557\n",
      "Epoch 6/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.6705 - accuracy: 0.5759 - val_loss: 1.6837 - val_accuracy: 0.5793\n",
      "Epoch 7/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.5859 - accuracy: 0.5992 - val_loss: 1.6518 - val_accuracy: 0.5833\n",
      "Epoch 8/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.5152 - accuracy: 0.6186 - val_loss: 1.5502 - val_accuracy: 0.6126\n",
      "Epoch 9/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.4032 - accuracy: 0.6505 - val_loss: 1.4786 - val_accuracy: 0.6322\n",
      "Epoch 10/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.3133 - accuracy: 0.6706 - val_loss: 1.4085 - val_accuracy: 0.6438\n",
      "Epoch 11/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.2432 - accuracy: 0.6889 - val_loss: 1.3555 - val_accuracy: 0.6545\n",
      "Epoch 12/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.1840 - accuracy: 0.6990 - val_loss: 1.3001 - val_accuracy: 0.6723\n",
      "Epoch 13/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.1011 - accuracy: 0.7197 - val_loss: 1.3005 - val_accuracy: 0.6834\n",
      "Epoch 14/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.0514 - accuracy: 0.7358 - val_loss: 1.2834 - val_accuracy: 0.6901\n",
      "Epoch 15/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.0300 - accuracy: 0.7420 - val_loss: 1.2896 - val_accuracy: 0.6834\n",
      "Epoch 16/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.9714 - accuracy: 0.7555 - val_loss: 1.2377 - val_accuracy: 0.6910\n",
      "Epoch 17/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.9210 - accuracy: 0.7691 - val_loss: 1.2565 - val_accuracy: 0.6915\n",
      "Epoch 18/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.9047 - accuracy: 0.7735 - val_loss: 1.2347 - val_accuracy: 0.7079\n",
      "Epoch 19/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.8556 - accuracy: 0.7822 - val_loss: 1.2379 - val_accuracy: 0.6986\n",
      "Epoch 20/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.8246 - accuracy: 0.7871 - val_loss: 1.2232 - val_accuracy: 0.7093\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=100, epochs=20, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
