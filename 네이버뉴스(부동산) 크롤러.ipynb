{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estate_news():\n",
    "    \n",
    "    \n",
    "    # 부동산 뉴스 기본 url\n",
    "    url_base = 'https://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid2=260&sid1=101&date='\n",
    "    \n",
    "    # 기사 작성일 기준 전날부터 일주일을 분석하기 위해 날짜 생성\n",
    "    dates = []\n",
    "    for i in range(7):\n",
    "        dates.append((date.today()- timedelta(days = i+1)).strftime(\"%Y%m%d\"))\n",
    "    \n",
    "    \n",
    "    # 기사 크롤러 시작   :  일자별(7일), 페이지별(1 ~ 마지막페이지), 각 기사별 크롤링. 순서는 역순.\n",
    "    title, content = [], []\n",
    "    for dt in dates:   # 7일 반복\n",
    "        first_url = url_base + dt + '&page=1' # 첫번째 일자에 첫 페이지 읽기\n",
    "        first_html = urlopen(first_url).read()  # xml로 파싱하기 위함\n",
    "        first_soup = bs(first_html, 'html.parser') # 페이지 파싱\n",
    "\n",
    "        first_pages = first_soup.find('div',{'class':'paging'})  # 하단의 페이지 읽기 ex) '1', '2', '3' ...\n",
    "        if first_pages.text[-3:-1] =='다음': # 10 페이지 이상일 경우 다음을 눌러야 11페이지가 나옴.\n",
    "            first_page_max = 10 # 다음이 있을땐 현재 페이지에선 10페이지가 마지막\n",
    "            \n",
    "            for page in range(1,first_page_max+1): # 10페이지가 넘어갈 경우 반복하여 1~10페이지 파싱\n",
    "                url = url_base + dt + '&page=' + str(page) # 각 페이지별 url 주소 입력\n",
    "                html = urlopen(url).read() # 각 페이지별 url 읽기.\n",
    "                xmlsoup = bs(html, 'html.parser') # 파싱\n",
    "\n",
    "                \n",
    "                lis = xmlsoup.select('#main_content > div.list_body.newsflash_body > \\\n",
    "                                    ul.type06_headline > li') # 기사 list\n",
    "                \n",
    "                for i in lis: # list 내 기사 링크로 이동하여 제목과 내용 크롤링\n",
    "                    link = i.find('a', attrs = {'href':re.compile('^https://')}).get('href')\n",
    "                    news = urlopen(link).read()\n",
    "                    newssoup = bs(news, 'html.parser')\n",
    "\n",
    "                    title.append(newssoup.select('#articleTitle')[0].text)\n",
    "                    content.append(newssoup.select('#articleBodyContents')[0].text)\n",
    "            \n",
    "            # 10페이지 이후 다음페이지로 넘어가서 11페이지부터 위 작업 반복하기\n",
    "            second_url = 'https://news.naver.com/main/list.nhn' + first_pages.find('a',{'class':'next'}).get('href')\n",
    "            second_html = urlopen(second_url).read()\n",
    "            second_soup = bs(second_html, 'html.parser')\n",
    "            \n",
    "            second_pages = second_soup.find('div',{'class':'paging'})  # 하단의 페이지 읽기 ex) '1', '2', '3' ...\n",
    "            if second_pages.text[-3:-1] =='다음': # 11-20에서 다음이 존재할 경우 (21p) 다음으로 넘어가기 위함\n",
    "                second_page_max = 20 # 다음이 있을땐 현재 페이지에선 20페이지가 마지막\n",
    "\n",
    "                for page in range(11,second_page_max+1):\n",
    "                    url = url_base + dt + '&page=' + str(page)\n",
    "                    html = urlopen(url).read()\n",
    "                    xmlsoup = bs(html, 'html.parser') \n",
    "\n",
    "                    lis = xmlsoup.select('#main_content > div.list_body.newsflash_body > ul.type06_headline > li')\n",
    "\n",
    "                    for i in lis:\n",
    "                        link = i.find('a', attrs = {'href':re.compile('^https://')}).get('href')\n",
    "                        news = urlopen(link).read()\n",
    "                        newssoup = bs(news, 'html.parser')\n",
    "\n",
    "                        title.append(newssoup.select('#articleTitle')[0].text)\n",
    "                        content.append(newssoup.select('#articleBodyContents')[0].text)\n",
    "                \n",
    "                third_url = 'https://news.naver.com/main/list.nhn' + second_pages.find('a',{'class':'next'}).get('href')\n",
    "                third_html = urlopen(third_url).read()\n",
    "                third_soup = bs(third_html, 'html.parser')\n",
    "            \n",
    "                third_pages = third_soup.find('div',{'class':'paging'})  # 하단의 페이지 읽기 ex) '1', '2', '3' ...\n",
    "                try:       \n",
    "                    third_page_max = int(third_pages.text[-3:-1])\n",
    "                    for page in range(21,third_page_max+1):\n",
    "                        url = url_base + dt + '&page=' + str(page)\n",
    "                        html = urlopen(url).read()\n",
    "                        xmlsoup = bs(html, 'html.parser') \n",
    "\n",
    "                        lis = xmlsoup.select('#main_content > div.list_body.newsflash_body > ul.type06_headline > li')\n",
    "\n",
    "                        for i in lis:\n",
    "                            link = i.find('a', attrs = {'href':re.compile('^https://')}).get('href')\n",
    "                            news = urlopen(link).read()\n",
    "                            newssoup = bs(news, 'html.parser')\n",
    "\n",
    "                            title.append(newssoup.select('#articleTitle')[0].text)\n",
    "                            content.append(newssoup.select('#articleBodyContents')[0].text)\n",
    "                except:\n",
    "                    print('{} 날짜는 31페이지 이상이 존재합니다'.format(dt))\n",
    "                \n",
    "            else:\n",
    "                second_page_max = int(second_pages.text[-3:-1])\n",
    "                \n",
    "                for page in range(11,second_page_max+1):\n",
    "                    url = url_base + dt + '&page=' + str(page)\n",
    "                    html = urlopen(url).read()\n",
    "                    xmlsoup = bs(html, 'html.parser') \n",
    "\n",
    "                    lis = xmlsoup.select('#main_content > div.list_body.newsflash_body > ul.type06_headline > li')\n",
    "\n",
    "                    for i in lis:\n",
    "                        link = i.find('a', attrs = {'href':re.compile('^https://')}).get('href')\n",
    "                        news = urlopen(link).read()\n",
    "                        newssoup = bs(news, 'html.parser')\n",
    "\n",
    "                        title.append(newssoup.select('#articleTitle')[0].text)\n",
    "                        content.append(newssoup.select('#articleBodyContents')[0].text)\n",
    "\n",
    "        else: \n",
    "            first_page_max = int(first_pages.text[-2])\n",
    "\n",
    "            \n",
    "        for page in range(1,first_page_max+1):\n",
    "            url = url_base + dt + '&page=' + str(page)\n",
    "            html = urlopen(url).read()\n",
    "            xmlsoup = bs(html, 'html.parser') \n",
    "\n",
    "            lis = xmlsoup.select('#main_content > div.list_body.newsflash_body > ul.type06_headline > li')\n",
    "\n",
    "            for i in lis:\n",
    "                link = i.find('a', attrs = {'href':re.compile('^https://')}).get('href')\n",
    "                news = urlopen(link).read()\n",
    "                newssoup = bs(news, 'html.parser')\n",
    "\n",
    "                title.append(newssoup.select('#articleTitle')[0].text)\n",
    "                content.append(newssoup.select('#articleBodyContents')[0].text)\n",
    "        \n",
    "\n",
    "    df_news = pd.DataFrame({'title':title, 'content':content})\n",
    "    df_news['content'] = df_news['content'].apply(lambda x : ' '.join(x.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\").split()))\n",
    "    \n",
    "    return df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327.16304898262024\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_news = get_estate_news()\n",
    "\n",
    "end = time.time() - start\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['content'] = df_news['content'].apply(lambda x : ' '.join(x.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\").split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>서울 아파트 지역별 온도차 뚜렷…  강남권 매수심리 위축</td>\n",
       "      <td>[파이낸셜뉴스] 고가주택에 대한 대출규제를 담은 12.16대책 이후 서울 아파트 시...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12·16대책 이후 훈풍 분다던 오피스텔...작년 대비 거래량 9.9% 감소해</td>\n",
       "      <td>서울 송파구 문정동 오피스텔 전경. 자료=상가정보연구소 [파이낸셜뉴스] 고가 아파트...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[강신우의 하우쓱]“LH공공임대아파트 ‘로또분양’ 하나”</td>\n",
       "      <td>LH vs 입주민 분양가 ‘갈등’LH “감정평가액 100%가 분양가”주민 “5년임대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2월 셋째 주] 코로나19 불안에도 분양물량 본격 풀린다</td>\n",
       "      <td>[파이낸셜뉴스] 2월 셋째 주에는 청약 작업 이관과 코로나19로 움츠렸던 부동산 시...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>롯데캐슬 현장, 새해부터 자발적 봉사활동</td>\n",
       "      <td>주안 캐슬&amp;더샵 봉사단이 환경정화를 위해 쓰레기를 줍고 있다./ 사진=롯데건설 제공...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>[아파트 돋보기]공동주택 관리도 전문가가 필요하다①</td>\n",
       "      <td>경기도 파주 운정신도시 아파트 단지(사진=이데일리DB)[이데일리 김용운 기자] 우리...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>코로나사태 길어지면…항공·정유화학↓ 디스플레이↑</td>\n",
       "      <td>(자료=유진투자증권)[서울=뉴시스] 박주연 기자 = 중국을 강타한 신종 코로나 바이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>`오매불망` 과천 지식정보타운 첫 분양…당첨자 선정 기준 보니</td>\n",
       "      <td>`과천 제이드 자이`(S9블록) 이달 공급해야 최소 청약거주기간 1년 적용청약통장 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>“이럴 줄 알았음 안 들어와” 청년 울리는 청년임대주택 [밀착취재]</td>\n",
       "      <td>文정부 공약 '청년 임대주택', 공급 확대 치중해 관리 소홀 비판 / 입주민 \"쓰레...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>[역세권 상권분석] 사당역 하루 승하차 '15만명'</td>\n",
       "      <td>[역세권 상권분석] ④-사당역사당역은 지하철 하루 승하차 이용자수가 평균 약 15만...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1426 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title  \\\n",
       "0                 서울 아파트 지역별 온도차 뚜렷…  강남권 매수심리 위축   \n",
       "1     12·16대책 이후 훈풍 분다던 오피스텔...작년 대비 거래량 9.9% 감소해   \n",
       "2                 [강신우의 하우쓱]“LH공공임대아파트 ‘로또분양’ 하나”   \n",
       "3                [2월 셋째 주] 코로나19 불안에도 분양물량 본격 풀린다   \n",
       "4                          롯데캐슬 현장, 새해부터 자발적 봉사활동   \n",
       "...                                           ...   \n",
       "1421                 [아파트 돋보기]공동주택 관리도 전문가가 필요하다①   \n",
       "1422                   코로나사태 길어지면…항공·정유화학↓ 디스플레이↑   \n",
       "1423           `오매불망` 과천 지식정보타운 첫 분양…당첨자 선정 기준 보니   \n",
       "1424        “이럴 줄 알았음 안 들어와” 청년 울리는 청년임대주택 [밀착취재]   \n",
       "1425                 [역세권 상권분석] 사당역 하루 승하차 '15만명'   \n",
       "\n",
       "                                                content  \n",
       "0     [파이낸셜뉴스] 고가주택에 대한 대출규제를 담은 12.16대책 이후 서울 아파트 시...  \n",
       "1     서울 송파구 문정동 오피스텔 전경. 자료=상가정보연구소 [파이낸셜뉴스] 고가 아파트...  \n",
       "2     LH vs 입주민 분양가 ‘갈등’LH “감정평가액 100%가 분양가”주민 “5년임대...  \n",
       "3     [파이낸셜뉴스] 2월 셋째 주에는 청약 작업 이관과 코로나19로 움츠렸던 부동산 시...  \n",
       "4     주안 캐슬&더샵 봉사단이 환경정화를 위해 쓰레기를 줍고 있다./ 사진=롯데건설 제공...  \n",
       "...                                                 ...  \n",
       "1421  경기도 파주 운정신도시 아파트 단지(사진=이데일리DB)[이데일리 김용운 기자] 우리...  \n",
       "1422  (자료=유진투자증권)[서울=뉴시스] 박주연 기자 = 중국을 강타한 신종 코로나 바이...  \n",
       "1423  `과천 제이드 자이`(S9블록) 이달 공급해야 최소 청약거주기간 1년 적용청약통장 ...  \n",
       "1424  文정부 공약 '청년 임대주택', 공급 확대 치중해 관리 소홀 비판 / 입주민 \"쓰레...  \n",
       "1425  [역세권 상권분석] ④-사당역사당역은 지하철 하루 승하차 이용자수가 평균 약 15만...  \n",
       "\n",
       "[1426 rows x 2 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
