{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Kosmo_24'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd # CTRL + SHIFT + - = 셀 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목표 : 레드와인, 화이트와인 구분하기\n",
    "    \n",
    "데이터 : UCI repository 데이터 \n",
    "    - 6497개 행\n",
    "    - 레드와인 1599개, 화이트 와인이 4896개\n",
    "    - 13개 컬럼\n",
    "    - 12개 feature : 주석산 농도, 아세트산 농도, 구연산 농도, 잔류 당분 농도, 염화나트륨 농도, 유리 아황산 농도, 총 아황산 농도,\n",
    "                    밀도, PH, 황산칼륨 농도, 알코올 도수, 와인 맛(0~10등급)\n",
    "    - class : 레드와인(1), 화이트와인(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed값 설정\n",
    "\n",
    "seed=0\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6492</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6493</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6494</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6495</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6496</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 입력\n",
    "\n",
    "df_pre = pd.read_csv('c:/chosungeun/data/dataset/dataset/wine.csv', header=None)\n",
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5316</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.053</td>\n",
       "      <td>20.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.99373</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5210</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.047</td>\n",
       "      <td>30.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.99164</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.54</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3518</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.50</td>\n",
       "      <td>13.70</td>\n",
       "      <td>0.049</td>\n",
       "      <td>56.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.99940</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.66</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1622</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.074</td>\n",
       "      <td>25.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99370</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2443</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.034</td>\n",
       "      <td>29.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99170</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2491</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.036</td>\n",
       "      <td>38.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.99280</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.48</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3561</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.041</td>\n",
       "      <td>11.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.99580</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4805</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.39</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.031</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.99130</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>12.9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3045</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.063</td>\n",
       "      <td>35.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99110</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.42</td>\n",
       "      <td>12.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3138</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.028</td>\n",
       "      <td>32.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.99360</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.54</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3248 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2      3      4     5      6        7     8     9     10  \\\n",
       "5316  6.3  0.18  0.24   3.40  0.053  20.0  119.0  0.99373  3.11  0.52   9.2   \n",
       "5210  6.8  0.14  0.18   1.40  0.047  30.0   90.0  0.99164  3.27  0.54  11.2   \n",
       "3518  7.3  0.22  0.50  13.70  0.049  56.0  189.0  0.99940  3.24  0.66   9.0   \n",
       "1622  7.6  0.67  0.14   1.50  0.074  25.0  168.0  0.99370  3.05  0.51   9.3   \n",
       "2443  7.3  0.21  0.29   1.60  0.034  29.0  118.0  0.99170  3.30  0.50  11.0   \n",
       "...   ...   ...   ...    ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "2491  5.9  0.26  0.30   1.00  0.036  38.0  114.0  0.99280  3.58  0.48   9.4   \n",
       "3561  8.2  0.21  0.48   1.40  0.041  11.0   99.0  0.99580  3.17  0.57   9.9   \n",
       "4805  6.6  0.29  0.39   6.75  0.031  22.0   98.0  0.99130  3.15  0.80  12.9   \n",
       "3045  6.3  0.27  0.49   1.20  0.063  35.0   92.0  0.99110  3.38  0.42  12.2   \n",
       "3138  7.3  0.26  0.49   5.00  0.028  32.0  107.0  0.99360  3.24  0.54  10.8   \n",
       "\n",
       "      11  12  \n",
       "5316   6   0  \n",
       "5210   6   0  \n",
       "3518   6   0  \n",
       "1622   5   0  \n",
       "2443   8   0  \n",
       "...   ..  ..  \n",
       "2491   5   0  \n",
       "3561   5   0  \n",
       "4805   7   0  \n",
       "3045   6   0  \n",
       "3138   6   0  \n",
       "\n",
       "[3248 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 샘플링\n",
    "df = df_pre.sample(frac=0.5) # 랜덤 샘플을 가져오는 50%만 가져오겠다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.3 ,  0.18,  0.24, ...,  9.2 ,  6.  ,  0.  ],\n",
       "       [ 6.8 ,  0.14,  0.18, ..., 11.2 ,  6.  ,  0.  ],\n",
       "       [ 7.3 ,  0.22,  0.5 , ...,  9.  ,  6.  ,  0.  ],\n",
       "       ...,\n",
       "       [ 6.6 ,  0.29,  0.39, ..., 12.9 ,  7.  ,  0.  ],\n",
       "       [ 6.3 ,  0.27,  0.49, ..., 12.2 ,  6.  ,  0.  ],\n",
       "       [ 7.3 ,  0.26,  0.49, ..., 10.8 ,  6.  ,  0.  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.values\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:, 0:12] # 와인의 feature\n",
    "Y = dataset[:, 12] # 레드(1)인지 화이트(0)인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3248, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.3 ,  0.18,  0.24, ...,  0.52,  9.2 ,  6.  ],\n",
       "       [ 6.8 ,  0.14,  0.18, ...,  0.54, 11.2 ,  6.  ],\n",
       "       [ 7.3 ,  0.22,  0.5 , ...,  0.66,  9.  ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.6 ,  0.29,  0.39, ...,  0.8 , 12.9 ,  7.  ],\n",
       "       [ 6.3 ,  0.27,  0.49, ...,  0.42, 12.2 ,  6.  ],\n",
       "       [ 7.3 ,  0.26,  0.49, ...,  0.54, 10.8 ,  6.  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3248,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 구조 짜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(32, input_dim=12, activation='relu')) # 인풋값(12), 히든레이어1\n",
    "model.add(Dense(16, activation='relu'))               # 히든레이어2\n",
    "model.add(Dense(8, activation='relu'))                # 히든레이어3\n",
    "model.add(Dense(1, activation='sigmoid'))             # 아웃풋 레이어(이진 분류)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 체크포인트 만들어서 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR=\"./model/\"\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "# 모델 저장 조건 설정\n",
    "\n",
    "modelpath='./model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earlystopping 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과적합 방지하려고 사용 / \n",
    "train set에만 overfitting된 모델을 만들지 않고 test set에도 잘 작동하는 모델을 만들기 위함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 자동중단 설정\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71098, saving model to ./model/01-0.7110.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71098 to 0.42179, saving model to ./model/02-0.4218.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.42179 to 0.38133, saving model to ./model/03-0.3813.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38133 to 0.35995, saving model to ./model/04-0.3600.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35995 to 0.35093, saving model to ./model/05-0.3509.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35093 to 0.29700, saving model to ./model/06-0.2970.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.29700 to 0.27305, saving model to ./model/07-0.2731.hdf5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.27305\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.27305 to 0.24875, saving model to ./model/09-0.2488.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.24875 to 0.24268, saving model to ./model/10-0.2427.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.24268 to 0.23269, saving model to ./model/11-0.2327.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.23269 to 0.22839, saving model to ./model/12-0.2284.hdf5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.22839\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.22839 to 0.22765, saving model to ./model/14-0.2277.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.22765 to 0.22028, saving model to ./model/15-0.2203.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.22028 to 0.21444, saving model to ./model/16-0.2144.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.21444 to 0.21117, saving model to ./model/17-0.2112.hdf5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.21117\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.21117\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.21117 to 0.20566, saving model to ./model/20-0.2057.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.20566 to 0.20245, saving model to ./model/21-0.2025.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.20245 to 0.19923, saving model to ./model/22-0.1992.hdf5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.19923\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.19923 to 0.19630, saving model to ./model/24-0.1963.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.19630 to 0.19280, saving model to ./model/25-0.1928.hdf5\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.19280 to 0.19257, saving model to ./model/26-0.1926.hdf5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.19257\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.19257\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.19257 to 0.18875, saving model to ./model/29-0.1887.hdf5\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.18875 to 0.18363, saving model to ./model/30-0.1836.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.18363 to 0.18264, saving model to ./model/31-0.1826.hdf5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.18264 to 0.18017, saving model to ./model/32-0.1802.hdf5\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.18017 to 0.17802, saving model to ./model/33-0.1780.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.17802 to 0.17646, saving model to ./model/34-0.1765.hdf5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.17646\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.17646 to 0.17286, saving model to ./model/36-0.1729.hdf5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.17286\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.17286\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.17286\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.17286 to 0.16515, saving model to ./model/40-0.1652.hdf5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.16515\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.16515\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.16515\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.16515\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.16515\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.16515 to 0.16068, saving model to ./model/46-0.1607.hdf5\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.16068 to 0.15397, saving model to ./model/47-0.1540.hdf5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.15397\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.15397 to 0.15308, saving model to ./model/49-0.1531.hdf5\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.15308 to 0.15160, saving model to ./model/50-0.1516.hdf5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.15160\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.15160 to 0.15145, saving model to ./model/52-0.1514.hdf5\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.15145 to 0.14054, saving model to ./model/53-0.1405.hdf5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.14054\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.14054 to 0.13942, saving model to ./model/55-0.1394.hdf5\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.13942 to 0.13514, saving model to ./model/56-0.1351.hdf5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.13514\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.13514 to 0.13061, saving model to ./model/58-0.1306.hdf5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.13061\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.13061 to 0.12865, saving model to ./model/60-0.1286.hdf5\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.12865 to 0.12600, saving model to ./model/61-0.1260.hdf5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.12600\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.12600\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.12600 to 0.12464, saving model to ./model/64-0.1246.hdf5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.12464\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.12464\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.12464\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.12464 to 0.12019, saving model to ./model/68-0.1202.hdf5\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.12019 to 0.11771, saving model to ./model/69-0.1177.hdf5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.11771\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.11771 to 0.11635, saving model to ./model/71-0.1164.hdf5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.11635\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.11635 to 0.11435, saving model to ./model/73-0.1144.hdf5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.11435 to 0.11393, saving model to ./model/74-0.1139.hdf5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.11393\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.11393 to 0.11201, saving model to ./model/76-0.1120.hdf5\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.11201 to 0.11177, saving model to ./model/77-0.1118.hdf5\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.11177\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.11177 to 0.10908, saving model to ./model/79-0.1091.hdf5\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10908\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.10908 to 0.10752, saving model to ./model/81-0.1075.hdf5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10752\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.10752 to 0.10713, saving model to ./model/83-0.1071.hdf5\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.10713\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.10713 to 0.10552, saving model to ./model/85-0.1055.hdf5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.10552 to 0.10508, saving model to ./model/86-0.1051.hdf5\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.10508 to 0.10449, saving model to ./model/87-0.1045.hdf5\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.10449 to 0.10374, saving model to ./model/88-0.1037.hdf5\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.10374 to 0.10190, saving model to ./model/89-0.1019.hdf5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.10190\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.10190 to 0.10082, saving model to ./model/91-0.1008.hdf5\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.10082 to 0.09967, saving model to ./model/92-0.0997.hdf5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.09967\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.09967\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.09967\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.09967 to 0.09919, saving model to ./model/96-0.0992.hdf5\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.09919 to 0.09894, saving model to ./model/97-0.0989.hdf5\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.09894\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.09894 to 0.09538, saving model to ./model/99-0.0954.hdf5\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.09538\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.09538 to 0.09410, saving model to ./model/101-0.0941.hdf5\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.09410\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.09410 to 0.09298, saving model to ./model/103-0.0930.hdf5\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.09298\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.09298 to 0.09141, saving model to ./model/105-0.0914.hdf5\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.09141\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.09141\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.09141\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.09141 to 0.08987, saving model to ./model/109-0.0899.hdf5\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.08987\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.08987 to 0.08955, saving model to ./model/111-0.0896.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00112: val_loss did not improve from 0.08955\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.08955\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.08955\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.08955 to 0.08716, saving model to ./model/115-0.0872.hdf5\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.08716 to 0.08629, saving model to ./model/116-0.0863.hdf5\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.08629 to 0.08539, saving model to ./model/117-0.0854.hdf5\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.08539\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.08539\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.08539 to 0.08433, saving model to ./model/120-0.0843.hdf5\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.08433\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.08433 to 0.08428, saving model to ./model/122-0.0843.hdf5\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.08428\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.08428 to 0.08327, saving model to ./model/124-0.0833.hdf5\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.08327\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.08327\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.08327 to 0.08320, saving model to ./model/127-0.0832.hdf5\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.08320 to 0.08246, saving model to ./model/128-0.0825.hdf5\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.08246 to 0.08116, saving model to ./model/129-0.0812.hdf5\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.08116 to 0.08046, saving model to ./model/132-0.0805.hdf5\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.08046\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.08046\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.08046\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.08046\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.08046 to 0.07899, saving model to ./model/137-0.0790.hdf5\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.07899\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.07899 to 0.07798, saving model to ./model/139-0.0780.hdf5\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.07798\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.07798\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.07798\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.07798\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.07798 to 0.07698, saving model to ./model/144-0.0770.hdf5\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.07698\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.07698 to 0.07629, saving model to ./model/146-0.0763.hdf5\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.07629\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.07629\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.07629\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.07629\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.07629 to 0.07594, saving model to ./model/151-0.0759.hdf5\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.07594\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.07594 to 0.07570, saving model to ./model/153-0.0757.hdf5\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.07570\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.07570 to 0.07386, saving model to ./model/155-0.0739.hdf5\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.07386\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.07386\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.07386 to 0.07358, saving model to ./model/158-0.0736.hdf5\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.07358\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.07358\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.07358\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.07358\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.07358\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.07358\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.07358 to 0.07309, saving model to ./model/165-0.0731.hdf5\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.07309\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.07309 to 0.07262, saving model to ./model/167-0.0726.hdf5\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.07262\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.07262\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.07262\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.07262\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.07262 to 0.07260, saving model to ./model/172-0.0726.hdf5\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.07260\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.07260\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.07260 to 0.07198, saving model to ./model/175-0.0720.hdf5\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.07198 to 0.07180, saving model to ./model/176-0.0718.hdf5\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.07180\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.07180 to 0.07164, saving model to ./model/181-0.0716.hdf5\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.07164 to 0.07104, saving model to ./model/182-0.0710.hdf5\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.07104\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.07104\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.07104\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.07104\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.07104\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.07104\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.07104\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.07104\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.07104 to 0.07082, saving model to ./model/191-0.0708.hdf5\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.07082\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.07082\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.07082\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.07082\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.07082\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.07082\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.07082\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.07082\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.07082\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.07082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26ce41fbb08>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 실행\n",
    "\n",
    "model.fit(X, Y, validation_split=0.2,\n",
    "          epochs=500, batch_size=200, verbose=0,\n",
    "          callbacks=[checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그래프로 테스트셋 오차, 학습셋 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2273 samples, validate on 975 samples\n",
      "Epoch 1/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0448 - accuracy: 0.9837 - val_loss: 0.0618 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.07082 to 0.06182, saving model to ./model/01-0.0618.hdf5\n",
      "Epoch 2/500\n",
      "2273/2273 [==============================] - 0s 9us/step - loss: 0.0406 - accuracy: 0.9868 - val_loss: 0.0696 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.06182\n",
      "Epoch 3/500\n",
      "2273/2273 [==============================] - 0s 7us/step - loss: 0.0494 - accuracy: 0.9833 - val_loss: 0.0702 - val_accuracy: 0.9754\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.06182\n",
      "Epoch 4/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0620 - accuracy: 0.9798 - val_loss: 0.0660 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.06182\n",
      "Epoch 5/500\n",
      "2273/2273 [==============================] - 0s 9us/step - loss: 0.0490 - accuracy: 0.9833 - val_loss: 0.0637 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.06182\n",
      "Epoch 6/500\n",
      "2273/2273 [==============================] - 0s 7us/step - loss: 0.0422 - accuracy: 0.9864 - val_loss: 0.0661 - val_accuracy: 0.9754\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06182\n",
      "Epoch 7/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0428 - accuracy: 0.9868 - val_loss: 0.0608 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06182 to 0.06084, saving model to ./model/07-0.0608.hdf5\n",
      "Epoch 8/500\n",
      "2273/2273 [==============================] - 0s 10us/step - loss: 0.0417 - accuracy: 0.9872 - val_loss: 0.0622 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.06084\n",
      "Epoch 9/500\n",
      "2273/2273 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.99 - 0s 8us/step - loss: 0.0412 - accuracy: 0.9855 - val_loss: 0.0618 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.06084\n",
      "Epoch 10/500\n",
      "2273/2273 [==============================] - 0s 7us/step - loss: 0.0430 - accuracy: 0.9868 - val_loss: 0.0608 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.06084\n",
      "Epoch 11/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0418 - accuracy: 0.9859 - val_loss: 0.0612 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.06084\n",
      "Epoch 12/500\n",
      "2273/2273 [==============================] - 0s 7us/step - loss: 0.0434 - accuracy: 0.9859 - val_loss: 0.0603 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06084 to 0.06032, saving model to ./model/12-0.0603.hdf5\n",
      "Epoch 13/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0419 - accuracy: 0.9872 - val_loss: 0.0627 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.06032\n",
      "Epoch 14/500\n",
      "2273/2273 [==============================] - 0s 9us/step - loss: 0.0428 - accuracy: 0.9881 - val_loss: 0.0662 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06032\n",
      "Epoch 15/500\n",
      "2273/2273 [==============================] - 0s 7us/step - loss: 0.0429 - accuracy: 0.9864 - val_loss: 0.0626 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06032\n",
      "Epoch 16/500\n",
      "2273/2273 [==============================] - 0s 9us/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 0.0601 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.06032 to 0.06011, saving model to ./model/16-0.0601.hdf5\n",
      "Epoch 17/500\n",
      "2273/2273 [==============================] - 0s 11us/step - loss: 0.0418 - accuracy: 0.9868 - val_loss: 0.0708 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06011\n",
      "Epoch 18/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0445 - accuracy: 0.9850 - val_loss: 0.0608 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06011\n",
      "Epoch 19/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0397 - accuracy: 0.9872 - val_loss: 0.0628 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.06011\n",
      "Epoch 20/500\n",
      "2273/2273 [==============================] - 0s 10us/step - loss: 0.0431 - accuracy: 0.9855 - val_loss: 0.0671 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06011\n",
      "Epoch 21/500\n",
      "2273/2273 [==============================] - 0s 9us/step - loss: 0.0404 - accuracy: 0.9877 - val_loss: 0.0598 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06011 to 0.05979, saving model to ./model/21-0.0598.hdf5\n",
      "Epoch 22/500\n",
      "2273/2273 [==============================] - 0s 9us/step - loss: 0.0399 - accuracy: 0.9890 - val_loss: 0.0604 - val_accuracy: 0.9754\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.05979\n",
      "Epoch 23/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0458 - accuracy: 0.9837 - val_loss: 0.0781 - val_accuracy: 0.9754\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.05979\n",
      "Epoch 24/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0491 - accuracy: 0.9868 - val_loss: 0.0595 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.05979 to 0.05951, saving model to ./model/24-0.0595.hdf5\n",
      "Epoch 25/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0413 - accuracy: 0.9881 - val_loss: 0.0627 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.05951\n",
      "Epoch 26/500\n",
      "2273/2273 [==============================] - 0s 9us/step - loss: 0.0432 - accuracy: 0.9833 - val_loss: 0.0684 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.05951\n",
      "Epoch 27/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0406 - accuracy: 0.9846 - val_loss: 0.0599 - val_accuracy: 0.9805\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.05951\n",
      "Epoch 28/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.0632 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.05951\n",
      "Epoch 29/500\n",
      "2273/2273 [==============================] - 0s 9us/step - loss: 0.0458 - accuracy: 0.9850 - val_loss: 0.0677 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.05951\n",
      "Epoch 30/500\n",
      "2273/2273 [==============================] - 0s 7us/step - loss: 0.0476 - accuracy: 0.9815 - val_loss: 0.0826 - val_accuracy: 0.9744\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.05951\n",
      "Epoch 31/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0533 - accuracy: 0.9811 - val_loss: 0.0676 - val_accuracy: 0.9764\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.05951\n",
      "Epoch 32/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0394 - accuracy: 0.9877 - val_loss: 0.0599 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.05951\n",
      "Epoch 33/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0411 - accuracy: 0.9868 - val_loss: 0.0607 - val_accuracy: 0.9795\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.05951\n",
      "Epoch 34/500\n",
      "2273/2273 [==============================] - 0s 8us/step - loss: 0.0438 - accuracy: 0.9850 - val_loss: 0.0656 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.05951\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X,Y, validation_split=0.3, epochs=500, batch_size=200, verbose=1, callbacks=[checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.06182038440154149,\n",
       "  0.0695712298918993,\n",
       "  0.07022877725271079,\n",
       "  0.06600778645429856,\n",
       "  0.06369361797204384,\n",
       "  0.06614239246417315,\n",
       "  0.060836777091026306,\n",
       "  0.062186218989201084,\n",
       "  0.06177081855443808,\n",
       "  0.06084796518851549,\n",
       "  0.06122806171576182,\n",
       "  0.06031917895262058,\n",
       "  0.06266023543400642,\n",
       "  0.06615863606715813,\n",
       "  0.06257300613782345,\n",
       "  0.06011229543349682,\n",
       "  0.07078678600299053,\n",
       "  0.06081268229545691,\n",
       "  0.06278271629260136,\n",
       "  0.06713120123514763,\n",
       "  0.05978629145866785,\n",
       "  0.06044834833114575,\n",
       "  0.07812174505148178,\n",
       "  0.05951460107014729,\n",
       "  0.06272506264921947,\n",
       "  0.06836145504927024,\n",
       "  0.059913853804270424,\n",
       "  0.06322656208888078,\n",
       "  0.06772687649115539,\n",
       "  0.08264714899735573,\n",
       "  0.0676413166981477,\n",
       "  0.05993343641360601,\n",
       "  0.060743300865093865,\n",
       "  0.06560971263127449],\n",
       " 'val_accuracy': [0.9764102697372437,\n",
       "  0.9764102697372437,\n",
       "  0.9753845930099487,\n",
       "  0.9774358868598938,\n",
       "  0.9764102697372437,\n",
       "  0.9753845930099487,\n",
       "  0.9794871807098389,\n",
       "  0.9784615635871887,\n",
       "  0.9794871807098389,\n",
       "  0.980512797832489,\n",
       "  0.9784615635871887,\n",
       "  0.980512797832489,\n",
       "  0.9794871807098389,\n",
       "  0.9774358868598938,\n",
       "  0.9784615635871887,\n",
       "  0.9815384745597839,\n",
       "  0.9784615635871887,\n",
       "  0.9794871807098389,\n",
       "  0.9794871807098389,\n",
       "  0.9764102697372437,\n",
       "  0.9794871807098389,\n",
       "  0.9753845930099487,\n",
       "  0.9753845930099487,\n",
       "  0.9794871807098389,\n",
       "  0.9774358868598938,\n",
       "  0.9764102697372437,\n",
       "  0.980512797832489,\n",
       "  0.9774358868598938,\n",
       "  0.9764102697372437,\n",
       "  0.9743589758872986,\n",
       "  0.9764102697372437,\n",
       "  0.9794871807098389,\n",
       "  0.9794871807098389,\n",
       "  0.9784615635871887],\n",
       " 'loss': [0.044796245025116026,\n",
       "  0.04060161451522705,\n",
       "  0.049352683379321804,\n",
       "  0.0619989522055506,\n",
       "  0.048961047556790496,\n",
       "  0.042218171145309714,\n",
       "  0.04282864468484409,\n",
       "  0.04172191656781774,\n",
       "  0.04121607427968362,\n",
       "  0.042986048353951485,\n",
       "  0.041844189575936004,\n",
       "  0.04339462205164505,\n",
       "  0.041899148943504994,\n",
       "  0.0428208936000914,\n",
       "  0.04292372760993502,\n",
       "  0.04085173149086335,\n",
       "  0.041791630452404555,\n",
       "  0.04446768840333202,\n",
       "  0.0396875683472275,\n",
       "  0.04307974248573846,\n",
       "  0.04044518574213489,\n",
       "  0.0398731590549798,\n",
       "  0.04577302629700905,\n",
       "  0.04906215051289859,\n",
       "  0.041312305909838386,\n",
       "  0.043227371651133516,\n",
       "  0.04063396438058126,\n",
       "  0.04135191087228739,\n",
       "  0.04583728237338184,\n",
       "  0.04758907791704031,\n",
       "  0.05331477639664848,\n",
       "  0.03944889728260156,\n",
       "  0.04106619007298719,\n",
       "  0.043835303751900104],\n",
       " 'accuracy': [0.983722,\n",
       "  0.98680156,\n",
       "  0.983282,\n",
       "  0.97976243,\n",
       "  0.983282,\n",
       "  0.9863616,\n",
       "  0.98680156,\n",
       "  0.9872415,\n",
       "  0.98548174,\n",
       "  0.98680156,\n",
       "  0.9859217,\n",
       "  0.9859217,\n",
       "  0.9872415,\n",
       "  0.98812145,\n",
       "  0.9863616,\n",
       "  0.9863616,\n",
       "  0.98680156,\n",
       "  0.9850418,\n",
       "  0.9872415,\n",
       "  0.98548174,\n",
       "  0.98768145,\n",
       "  0.98900133,\n",
       "  0.983722,\n",
       "  0.98680156,\n",
       "  0.98812145,\n",
       "  0.983282,\n",
       "  0.98460186,\n",
       "  0.9863616,\n",
       "  0.9850418,\n",
       "  0.9815222,\n",
       "  0.98108226,\n",
       "  0.98768145,\n",
       "  0.98680156,\n",
       "  0.9850418]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss=history.history['val_loss'] # 테스트셋 오차\n",
    "\n",
    "# print(len(y_vloss))\n",
    "# y_vloss\n",
    "\n",
    "# 학습셋 정확도\n",
    "\n",
    "y_acc = history.history['accuracy']\n",
    "# y_acc\n",
    "\n",
    "# 테스트셋 정확도\n",
    "\n",
    "y_val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# y_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9785\n"
     ]
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "\n",
    "# 학습셋 정확도 라인\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=1, label=\"Trainset_accuracy\")\n",
    "\n",
    "# 테스트셋 오차 라인\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=1, label='Testset_loss')\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"score\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 테스트 정확도 출력\n",
    "print(\"Accuracy : %.4f\" % y_val_accuracy[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
